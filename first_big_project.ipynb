{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0            6      148             72             35        0  33.6   \n",
      "1            1       85             66             29        0  26.6   \n",
      "2            8      183             64              0        0  23.3   \n",
      "3            1       89             66             23       94  28.1   \n",
      "4            0      137             40             35      168  43.1   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                     0.627   50        1  \n",
      "1                     0.351   31        0  \n",
      "2                     0.672   32        1  \n",
      "3                     0.167   21        0  \n",
      "4                     2.288   33        1  \n",
      "(768, 9)\n",
      "Pregnancies                 0\n",
      "Glucose                     0\n",
      "BloodPressure               0\n",
      "SkinThickness               0\n",
      "Insulin                     0\n",
      "BMI                         0\n",
      "DiabetesPedigreeFunction    0\n",
      "Age                         0\n",
      "Outcome                     0\n",
      "dtype: int64\n",
      "0    500\n",
      "1    268\n",
      "Name: Outcome, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a Pandas dataframe\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv', header=None)\n",
    "\n",
    "# Set column names for the dataframe\n",
    "df.columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "\n",
    "# Print the first 5 rows of the dataframe, the shape of the dataframe, the number of missing values in each column\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "print(df.isnull().sum())\n",
    "print(df['Outcome'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df.drop('Outcome', axis=1)\n",
    "y = df['Outcome']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features to have a mean of 0 and standard deviation of 1\n",
    "scaler = StandardScaler()\n",
    "X_train_ = scaler.fit_transform(X_train)\n",
    "X_test_ = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'C': 1, 'gamma': 0.01, 'kernel': 'linear'}\n",
      "Accuracy: 0.7467532467532467\n",
      "Precision: 0.6538461538461539\n",
      "Recall: 0.6181818181818182\n",
      "F1 score: 0.6355140186915889\n"
     ]
    }
   ],
   "source": [
    "model_choice = int(input('Enter 1: for Logistic Regression\\nEnter 2: for KNN\\nEnter 3: for SVM\\nEnter 4: for MLP: '))\n",
    "if model_choice == 1:\n",
    "    pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', SelectKBest(f_classif, k=5)),\n",
    "    ('logreg', LogisticRegression())\n",
    "    ])\n",
    "\n",
    "    # Define a parameter grid to search over\n",
    "    param_grid = {\n",
    "    'feature_selection__k': [5, 6, 7, 8],\n",
    "    'logreg__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "    }\n",
    "\n",
    "    # Perform grid search over the parameter grid\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv=5)\n",
    "    grid_search.fit(X_train_, y_train)\n",
    "\n",
    "    # Print the best hyperparameters found by grid search\n",
    "    print('Best hyperparameters:', grid_search.best_params_)\n",
    "\n",
    "    # Evaluate the performance of the model on the testing set using the best hyperparameters\n",
    "    y_pred = grid_search.predict(X_test_)\n",
    "    print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "    print('Precision:', precision_score(y_test, y_pred))\n",
    "    print('Recall:', recall_score(y_test, y_pred))\n",
    "    print('F1 score:', f1_score(y_test, y_pred))\n",
    "\n",
    "elif model_choice == 2:\n",
    "    # Define a KNN model\n",
    "    model = KNeighborsClassifier()\n",
    "\n",
    "    # Define a parameter grid to search over\n",
    "    param_grid = {'n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]}\n",
    "\n",
    "    # Perform grid search over the parameter grid\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "    grid_search.fit(X_train_, y_train)\n",
    "\n",
    "    # Print the best hyperparameters found by grid search\n",
    "    print('Best hyperparameters:', grid_search.best_params_)\n",
    "\n",
    "    # Evaluate the performance of the model on the testing set using the best hyperparameters\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "    print('Precision:', precision_score(y_test, y_pred))\n",
    "    print('Recall:', recall_score(y_test, y_pred))\n",
    "    print('F1 score:', f1_score(y_test, y_pred))\n",
    "\n",
    "elif model_choice == 3:\n",
    "    # Define a SelectKBest feature selector to select the top 5 features using the chi2 scoring function\n",
    "    selector = SelectKBest(chi2, k=5)\n",
    "    # # Define a SVM model\n",
    "    model = SVC()\n",
    "\n",
    "   # Define a parameter grid to search over\n",
    "    param_grid = {\n",
    "        'C': [1, 10],\n",
    "        'gamma': [0.01],\n",
    "        'kernel': ['linear']\n",
    "    }\n",
    "    # Perform grid search over the parameter grid\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best hyperparameters found by grid search\n",
    "    print('Best hyperparameters:', grid_search.best_params_)\n",
    "\n",
    "    # Fit the selector and scaler on the training data\n",
    "    X_train = selector.fit_transform(X_train, y_train)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Transform the testing data using the selector and scaler\n",
    "    X_test = selector.transform(X_test)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Train the SVC model on the selected and scaled features\n",
    "    model = grid_search.best_estimator_\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing set using the trained SVC model\n",
    "    y_pred = model.predict(X_test)\n",
    "    print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "    print('Precision:', precision_score(y_test, y_pred))\n",
    "    print('Recall:', recall_score(y_test, y_pred))\n",
    "    print('F1 score:', f1_score(y_test, y_pred))\n",
    "\n",
    "elif model_choice == 4:\n",
    "    # Define a pipeline that includes feature selection and MLP\n",
    "    model = MLPClassifier()\n",
    "    pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('select', SelectKBest(f_classif)),\n",
    "        ('mlp', model)\n",
    "    ])\n",
    "\n",
    "    # Define a parameter grid to search over\n",
    "    param_grid = {\n",
    "        'select__k': [5, 6, 7, 8],\n",
    "        'mlp__hidden_layer_sizes': [(8,), (16,), (32,)],\n",
    "        'mlp__activation': ['relu', 'tanh', 'logistic'],\n",
    "        'mlp__solver': ['adam', 'sgd', 'lbfgs'],\n",
    "        'mlp__max_iter': [100, 200, 500]\n",
    "    }\n",
    "\n",
    "    # Perform grid search over the parameter grid\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv=5)\n",
    "    grid_search.fit(X_train_, y_train)\n",
    "\n",
    "    # Print the best hyperparameters found by grid search\n",
    "    print('Best hyperparameters:', grid_search.best_params_)\n",
    "\n",
    "    # Make predictions on the testing set using the best MLP model found by grid search\n",
    "    y_pred = grid_search.predict(X_test_)\n",
    "\n",
    "    # Calculate the performance metrics of the best MLP model\n",
    "    print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "    print('Precision:', precision_score(y_test, y_pred))\n",
    "    print('Recall:', recall_score(y_test, y_pred))\n",
    "    print('F1 score:', f1_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab85d6174591eafacf63fbe6192a20ed093e4e76868e4a67cc51300d7f212c22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
